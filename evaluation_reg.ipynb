{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\"\"\"\n",
    "for json_file in os.listdir(\"dataset/jsonl/\"):\n",
    "    df = pd.concat([df, pd.read_json(\"dataset/jsonl/\" + json_file, lines=True)])\n",
    "    print(f\"Added {json_file} to dataframe.\")\n",
    "\"\"\"\n",
    "df = pd.read_json('dataset/jsonl/en-US.jsonl', lines=True)\n",
    "df[\"locale\"] = df[\"locale\"].apply(lambda x: x.split(\"-\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"preprocess_lang.ipynb\"\n",
    "%run -i \"preprocess_nolang.ipynb\"\n",
    "%run -i \"evaluation.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz.functoolz import pipe\n",
    "\n",
    "params = [\n",
    "    remove_punctuation,\n",
    "    lowercase,\n",
    "    tokenize,\n",
    "    apply_stemming,\n",
    "    encode_labels\n",
    "]\n",
    "\n",
    "df = pipe(\n",
    "    df,\n",
    "    *params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing dataset.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['utt'] = df['utt'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "print(f\"Finished preprocessing dataset.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized = vectorizer.fit_transform(df['utt'].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = [f'__{feature_name}' for feature_name in vectorizer.get_feature_names_out()]\n",
    "output_cols = ['intent']\n",
    "averages = [None, \"macro\", \"weighted\", \"micro\", \"samples\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_df = pd.DataFrame(vectorized.todense(), columns=input_cols)\n",
    "df = pd.concat([df, count_vect_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for MultinomialNB: {'model__alpha': 0.1}\n",
      "======================================================================\n",
      "Evaluation metrics for RandomizedSearchCV\n",
      "======================================================================\n",
      "RandomizedSearchCV's default score metric: 0.7541365405179443\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8158    0.9118    0.8611        34\n",
      "           1     0.8889    0.7619    0.8205        21\n",
      "           2     0.8718    0.8293    0.8500        41\n",
      "           3     1.0000    0.8182    0.9000        11\n",
      "           4     0.8529    0.9062    0.8788        32\n",
      "           5     1.0000    0.1667    0.2857         6\n",
      "           6     0.6111    0.8462    0.7097        13\n",
      "           7     0.7226    0.7857    0.7529       126\n",
      "           8     0.8354    0.9851    0.9041        67\n",
      "           9     0.8638    0.8804    0.8720       209\n",
      "          11     0.8986    0.8611    0.8794        72\n",
      "          12     0.8000    0.8000    0.8000        15\n",
      "          13     0.8409    0.8409    0.8409        88\n",
      "          14     0.5455    0.5000    0.5217        12\n",
      "          15     0.8760    0.9496    0.9113       119\n",
      "          16     0.6774    0.8077    0.7368        26\n",
      "          17     0.9217    0.9298    0.9258       114\n",
      "          18     1.0000    0.0000    0.0000         1\n",
      "          19     0.7083    0.8947    0.7907        19\n",
      "          20     0.5846    0.4497    0.5084       169\n",
      "          21     0.8889    0.9231    0.9057        26\n",
      "          22     0.9730    1.0000    0.9863        36\n",
      "          23     0.8250    0.9167    0.8684        36\n",
      "          24     0.7391    0.8095    0.7727        21\n",
      "          25     0.8444    0.8837    0.8636        43\n",
      "          26     1.0000    0.0000    0.0000         3\n",
      "          27     0.9167    0.8148    0.8627        27\n",
      "          28     0.8125    0.7222    0.7647        18\n",
      "          29     1.0000    0.5000    0.6667        10\n",
      "          30     0.8529    0.7436    0.7945        39\n",
      "          31     0.7255    0.7255    0.7255        51\n",
      "          32     0.9111    0.7885    0.8454        52\n",
      "          33     1.0000    0.0000    0.0000         4\n",
      "          34     0.8800    0.6111    0.7213        36\n",
      "          35     0.8333    0.8571    0.8451        35\n",
      "          36     1.0000    0.1667    0.2857         6\n",
      "          37     0.7895    0.8468    0.8171       124\n",
      "          38     0.8378    0.7561    0.7949        41\n",
      "          39     0.9310    0.7714    0.8438        35\n",
      "          40     0.8351    0.9205    0.8757       176\n",
      "          41     0.8906    0.9048    0.8976        63\n",
      "          42     0.8356    0.8472    0.8414        72\n",
      "          43     0.8974    0.8974    0.8974        39\n",
      "          44     0.8148    0.7719    0.7928        57\n",
      "          45     0.7556    0.7234    0.7391       141\n",
      "          46     0.7667    0.9200    0.8364        25\n",
      "          47     0.6857    0.9231    0.7869        26\n",
      "          48     0.6939    0.7907    0.7391        43\n",
      "          49     0.7429    0.8387    0.7879        31\n",
      "          50     0.6875    0.5500    0.6111        20\n",
      "          51     0.9200    0.8519    0.8846        81\n",
      "          52     0.7200    0.7200    0.7200        25\n",
      "          53     0.6923    0.8182    0.7500        22\n",
      "          54     0.9667    0.8286    0.8923        35\n",
      "          55     0.8222    0.7255    0.7708        51\n",
      "          56     0.8800    0.9565    0.9167        23\n",
      "          57     0.8889    0.9143    0.9014        35\n",
      "          58     0.7368    0.9333    0.8235        15\n",
      "          59     0.8957    0.9359    0.9154       156\n",
      "\n",
      "    accuracy                         0.8211      2974\n",
      "   macro avg     0.8340    0.7548    0.7541      2974\n",
      "weighted avg     0.8222    0.8211    0.8163      2974\n",
      "\n",
      "Accuracy: 0.8211\n",
      "F1-score: 0.7541\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = train_and_use_model(\n",
    "    MultinomialNB(),\n",
    "    {\n",
    "        \"model__alpha\": [0.1, 0.5, 1.0, 1.5, 2.0],\n",
    "    },\n",
    "    n_iter=5,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultinomialNB\n",
    "\n",
    "Best params:\n",
    "{\n",
    "    'model__alpha': 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "9 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\sklearn.py\", line 1490, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\training.py\", line 185, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\core.py\", line 1918, in update\n",
      "    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\core.py\", line 279, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: bad allocation\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\sklearn.py\", line 1471, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\sklearn.py\", line 448, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\sklearn.py\", line 908, in _create_dmatrix\n",
      "    return DMatrix(**kwargs, nthread=self.n_jobs)\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\core.py\", line 743, in __init__\n",
      "    handle, feature_names, feature_types = dispatch_data_backend(\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\data.py\", line 962, in dispatch_data_backend\n",
      "    return _from_numpy_array(data, missing, threads, feature_names, feature_types)\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\data.py\", line 213, in _from_numpy_array\n",
      "    _check_call(\n",
      "  File \"C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\core.py\", line 279, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: bad allocation\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\nrtc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:44:03] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"gamma\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"subsample\" } are not used.\n",
      "\n",
      "Best params for XGBClassifier: {'model__subsample': 0.7, 'model__n_estimators': 200, 'model__min_child_weight': 3, 'model__max_depth': 3, 'model__max_delta_step': 1.0, 'model__learning_rate': 0.05, 'model__gamma': 0.5, 'model__booster': 'gblinear'}\n",
      "======================================================================\n",
      "Evaluation metrics for RandomizedSearchCV\n",
      "======================================================================\n",
      "RandomizedSearchCV's default score metric: 0.7887292811188353\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9032    0.8235    0.8615        34\n",
      "           1     0.9474    0.8571    0.9000        21\n",
      "           2     0.7907    0.8293    0.8095        41\n",
      "           3     0.8182    0.8182    0.8182        11\n",
      "           4     0.8333    0.9375    0.8824        32\n",
      "           5     1.0000    0.3333    0.5000         6\n",
      "           6     0.6471    0.8462    0.7333        13\n",
      "           7     0.6042    0.6905    0.6444       126\n",
      "           8     0.7975    0.9403    0.8630        67\n",
      "           9     0.8333    0.8373    0.8353       209\n",
      "          11     0.9231    0.8333    0.8759        72\n",
      "          12     0.8462    0.7333    0.7857        15\n",
      "          13     0.8750    0.8750    0.8750        88\n",
      "          14     0.6667    0.5000    0.5714        12\n",
      "          15     0.9316    0.9160    0.9237       119\n",
      "          16     0.6538    0.6538    0.6538        26\n",
      "          17     0.8739    0.9123    0.8927       114\n",
      "          18     0.5000    1.0000    0.6667         1\n",
      "          19     0.8947    0.8947    0.8947        19\n",
      "          20     0.4456    0.5089    0.4751       169\n",
      "          21     1.0000    0.8846    0.9388        26\n",
      "          22     0.9000    1.0000    0.9474        36\n",
      "          23     0.9032    0.7778    0.8358        36\n",
      "          24     0.7500    0.8571    0.8000        21\n",
      "          25     0.9048    0.8837    0.8941        43\n",
      "          26     1.0000    0.6667    0.8000         3\n",
      "          27     0.8519    0.8519    0.8519        27\n",
      "          28     0.8824    0.8333    0.8571        18\n",
      "          29     1.0000    0.6000    0.7500        10\n",
      "          30     0.8056    0.7436    0.7733        39\n",
      "          31     0.6731    0.6863    0.6796        51\n",
      "          32     0.9318    0.7885    0.8542        52\n",
      "          33     0.7500    0.7500    0.7500         4\n",
      "          34     0.7879    0.7222    0.7536        36\n",
      "          35     0.6585    0.7714    0.7105        35\n",
      "          36     0.5000    0.5000    0.5000         6\n",
      "          37     0.8000    0.8065    0.8032       124\n",
      "          38     0.7895    0.7317    0.7595        41\n",
      "          39     0.9062    0.8286    0.8657        35\n",
      "          40     0.8721    0.8523    0.8621       176\n",
      "          41     0.8667    0.8254    0.8455        63\n",
      "          42     0.8400    0.8750    0.8571        72\n",
      "          43     0.8974    0.8974    0.8974        39\n",
      "          44     0.7636    0.7368    0.7500        57\n",
      "          45     0.6496    0.6312    0.6403       141\n",
      "          46     0.8333    0.8000    0.8163        25\n",
      "          47     0.8000    0.9231    0.8571        26\n",
      "          48     0.6522    0.6977    0.6742        43\n",
      "          49     0.8065    0.8065    0.8065        31\n",
      "          50     0.7143    0.5000    0.5882        20\n",
      "          51     0.9306    0.8272    0.8758        81\n",
      "          52     0.8182    0.7200    0.7660        25\n",
      "          53     0.8000    0.7273    0.7619        22\n",
      "          54     0.9355    0.8286    0.8788        35\n",
      "          55     0.7045    0.6078    0.6526        51\n",
      "          56     1.0000    0.9565    0.9778        23\n",
      "          57     0.8621    0.7143    0.7813        35\n",
      "          58     0.6250    1.0000    0.7692        15\n",
      "          59     0.8758    0.9038    0.8896       156\n",
      "\n",
      "    accuracy                         0.7939      2974\n",
      "   macro avg     0.8106    0.7840    0.7887      2974\n",
      "weighted avg     0.8011    0.7939    0.7952      2974\n",
      "\n",
      "Accuracy: 0.7939\n",
      "F1-score: 0.7887\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#import xgboost as xgb\n",
    "import xgboost as xgb\n",
    "\n",
    "xgboost = train_and_use_model(\n",
    "    xgb.XGBClassifier(),\n",
    "    {\n",
    "        \"model__n_estimators\": [100, 200, 300, 400, 500],\n",
    "        \"model__max_depth\": [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        \"model__learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "        \"model__booster\": [\"gbtree\", \"gblinear\", \"dart\"],\n",
    "        \"model__gamma\": [0, 0.25, 0.5, 1.0],\n",
    "        \"model__min_child_weight\": [1, 3, 5, 7],\n",
    "        \"model__max_delta_step\": [0, 0.2, 0.6, 1.0],\n",
    "        \"model__subsample\": [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    },\n",
    "    n_iter=3\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Best Params:\n",
    "{\n",
    "    'model__subsample': 0.7,\n",
    "    'model__n_estimators': 200, \n",
    "    'model__min_child_weight': 3, \n",
    "    'model__max_depth': 3, \n",
    "    'model__max_delta_step': 1.0, \n",
    "    'model__learning_rate': 0.05, \n",
    "    'model__gamma': 0.5, \n",
    "    'model__booster': 'gblinear'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = train_and_use_model(\n",
    "    KNeighborsClassifier(),\n",
    "    {\n",
    "        \"model__n_neighbors\": [3, 5, 11, 19],\n",
    "        \"model__weights\": [\"uniform\", \"distance\"],\n",
    "        \"model__algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "        \"model__leaf_size\": [30, 40, 50, 60, 70, 80, 90, 100],\n",
    "        \"model__p\": [1, 2],\n",
    "        \"model__metric\": [\"minkowski\", \"euclidean\", \"manhattan\"],\n",
    "    },\n",
    "    n_iter=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"df = pd.DataFrame()\n",
    "\n",
    "for json_file in os.listdir(\"dataset/jsonl/\"):\n",
    "    df = pd.concat([df, pd.read_json(\"dataset/jsonl/\" + json_file, lines=True)])\n",
    "    print(f\"Added {json_file} to dataframe.\")\n",
    "\n",
    "df[\"locale\"] = df[\"locale\"].apply(lambda x: x.split(\"-\")[0])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from toolz.functoolz import pipe\n",
    "\n",
    "params = [\n",
    "    remove_punctuation,\n",
    "    lowercase,\n",
    "    tokenize,\n",
    "    apply_stemming,\n",
    "    encode_labels\n",
    "]\n",
    "\n",
    "df = pipe(\n",
    "    df,\n",
    "    *params\n",
    ")\n",
    "\n",
    "df['utt'] = df['utt'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "print(f\"Finished preprocessing dataset.\\n\\n\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = TfidfTransformer()\n",
    "vectorized = vectorizer.fit_transform(df['utt'].to_list())\n",
    "input_cols = [f'__{feature_name}' for feature_name in vectorizer.get_feature_names_out()]\n",
    "output_cols = ['intent']\n",
    "averages = [None, \"macro\", \"weighted\", \"micro\", \"samples\"]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"training_inputs = df[df['partition'] != 'test'][input_cols].values\n",
    "testing_inputs = df[df['partition'] == 'test'][input_cols].values\n",
    "training_classes = df[df['partition'] != 'test'][output_cols].values\n",
    "testing_classes = df[df['partition'] == 'test'][output_cols].values\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.metrics import f1_score\n",
    "\n",
    "nb.fit(training_inputs, training_classes)\n",
    "score = nb.score(testing_inputs, testing_classes)\n",
    "f1_score = f1_score(testing_classes, nb.predict(testing_inputs), average=\"macro\")\n",
    "\n",
    "print(f\"Score: {score}\")\n",
    "print(f\"F1 Score: {f1_score}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"xgboost.fit(training_inputs, training_classes)\n",
    "score = xgboost.score(testing_inputs, testing_classes)\n",
    "f1_score = f1_score(testing_classes, xgboost.predict(testing_inputs), average=\"macro\")\n",
    "\n",
    "print(f\"Score: {score}\")\n",
    "print(f\"F1 Score: {f1_score}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"knn.fit(training_inputs, training_classes)\n",
    "score = knn.score(testing_inputs, testing_classes)\n",
    "f1_score = f1_score(testing_classes, knn.predict(testing_inputs), average=\"macro\")\n",
    "\n",
    "print(f\"Score: {score}\")\n",
    "print(f\"F1 Score: {f1_score}\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
