{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import stopwordsiso as sw\n",
    "\n",
    "LANG_MAP = {\n",
    "    \"ar\": \"arabic\",\n",
    "    \"da\": \"danish\",\n",
    "    \"nl\": \"dutch\",\n",
    "    \"en\": \"english\",\n",
    "    \"fi\": \"finnish\",\n",
    "    \"fr\": \"french\",\n",
    "    \"de\": \"german\",\n",
    "    \"hu\": \"hungarian\",\n",
    "    \"it\": \"italian\",\n",
    "    \"nb\": \"norwegian\",\n",
    "    \"pt\": \"portuguese\",\n",
    "    \"ro\": \"romanian\",\n",
    "    \"ru\": \"russian\",\n",
    "    \"es\": \"spanish\",\n",
    "    \"sv\": \"swedish\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmer(locale: str):\n",
    "    \"\"\"\n",
    "    Returns the approprate stemmer, if one exists\n",
    "    \"\"\"\n",
    "    if locale in LANG_MAP:\n",
    "        return SnowballStemmer(LANG_MAP[locale])\n",
    "    return lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(stemmer, utterance: str) -> str:\n",
    "    \"\"\"\n",
    "    Stems the utterance after tokenizing it\n",
    "    \"\"\"\n",
    "    return \" \".join([stemmer.stem(token) for token in word_tokenize(utterance)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemming(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply stemming when there is a language available\n",
    "    \"\"\"\n",
    "    return data.assign(\n",
    "        utt=data.apply(lambda x: stem(get_stemmer(x[\"locale\"]), x[\"utt\"]), axis=1)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df):\n",
    "    \"\"\"Remove stopwords from text\"\"\"\n",
    "    # if there's no utt_text column, raise an exception\n",
    "    if 'utt_text' not in df.columns:\n",
    "        raise Exception('It\\'s not possible to remove stopwords without tokenizing first.')\n",
    "    \n",
    "    # for each utt, get stopword list according to locale, and remove stopwords\n",
    "    for locale in df['locale'].unique():\n",
    "        sw_list = sw.stopwords(locale)\n",
    "        # for each utt in locale, remove stopwords\n",
    "        df.loc[df['locale'] == locale, 'utt'] = df.loc[df['locale'] == locale, 'utt'].apply(lambda x: [word for word in x if word not in sw_list])\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
