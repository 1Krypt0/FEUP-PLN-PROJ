{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "import pandas as pd\n",
    "import stopwordsiso as sw\n",
    "from nltk.stem import SnowballStemmer\n",
    "import simplemma\n",
    "\n",
    "LANG_MAP = {\n",
    "    \"ar\": \"arabic\",\n",
    "    \"da\": \"danish\",\n",
    "    \"nl\": \"dutch\",\n",
    "    \"en\": \"english\",\n",
    "    \"fi\": \"finnish\",\n",
    "    \"fr\": \"french\",\n",
    "    \"de\": \"german\",\n",
    "    \"hu\": \"hungarian\",\n",
    "    \"it\": \"italian\",\n",
    "    \"nb\": \"norwegian\",\n",
    "    \"pt\": \"portuguese\",\n",
    "    \"ro\": \"romanian\",\n",
    "    \"ru\": \"russian\",\n",
    "    \"es\": \"spanish\",\n",
    "    \"sv\": \"swedish\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmer(locale: str) -> Callable[[str], str]:\n",
    "    \"\"\"\n",
    "    Returns the appropriate stemmer, if one exists\n",
    "    \"\"\"\n",
    "    if locale in LANG_MAP:\n",
    "        return SnowballStemmer(LANG_MAP[locale]).stem\n",
    "    return lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(stemmer: Callable[[str], str], utterance: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Stems the utterance after tokenizing it\n",
    "    \"\"\"\n",
    "    if isinstance(utterance, str):\n",
    "        raise TypeError(\"Utterance must be tokenized to be stemmed.\")\n",
    "    return [stemmer(token) for token in utterance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemming(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply stemming when there is a language available\n",
    "    \"\"\"\n",
    "    # stem the utt column according to the language given by the locale column\n",
    "    data[\"utt\"] = data.apply(\n",
    "        lambda row: stem(get_stemmer(row[\"locale\"]), row[\"utt\"]), axis=1\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(lemmatizer: Callable[[str], str], utterance: list[str], language: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Lemmatizes the utterance after tokenizing it\n",
    "    \"\"\"\n",
    "    if isinstance(utterance, str):\n",
    "        raise TypeError(\"Utterance must be tokenized to be stemmed.\")\n",
    "    return [lemmatizer(token, lang=language) for token in utterance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemmatization(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply lemmatization when there is a language available\n",
    "    \"\"\"\n",
    "    # lemmatize the utt column according to the language given by the locale column\n",
    "    # if the language is not supported, utt will simply not be lemmatized\n",
    "    data[\"utt\"] = data.apply(\n",
    "        lambda row: lemmatize(simplemma.lemmatize, row[\"utt\"], row[\"locale\"]), axis=1\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove stopwords from text\"\"\"\n",
    "    # if there's no utt_text column, raise an exception\n",
    "    if \"utt_text\" not in data.columns:\n",
    "        raise Exception(\n",
    "            \"It's not possible to remove stopwords without tokenizing first.\"\n",
    "        )\n",
    "\n",
    "    # for each utt, get stopword list according to locale, and remove stopwords\n",
    "    for locale in data[\"locale\"].unique():\n",
    "        sw_list = sw.stopwords(locale)\n",
    "        # for each utt in locale, remove stopwords\n",
    "        data.loc[data[\"locale\"] == locale, \"utt\"] = data.loc[\n",
    "            data[\"locale\"] == locale, \"utt\"\n",
    "        ].apply(lambda x: [word for word in x if word not in sw_list])\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
