{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove punctuation from text\"\"\"\n",
    "    data[\"utt\"] = data[\"utt\"].str.replace(r\"[^\\w\\s]\",\"\", regex=True)\n",
    "    return data\n",
    "\n",
    "def lowercase(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Lowercase text\"\"\"\n",
    "    data[\"utt\"] = data[\"utt\"].str.lower()\n",
    "    return data\n",
    "\n",
    "def drop_cols(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    drop = [\"worker_id\", \"slot_method\", \"judgments\"]\n",
    "    return data.drop(drop, axis=1)\n",
    "\n",
    "def encode_labels(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Encode labels\"\"\"\n",
    "    le = LabelEncoder()\n",
    "    le.fit(data['intent'])\n",
    "    data['intent'] = le.transform(data['intent'])\n",
    "    return data, le\n",
    "\n",
    "def decode_labels(data: np.ndarray, le: LabelEncoder) -> np.ndarray:\n",
    "    \"\"\"Decode labels\"\"\"\n",
    "    data = le.inverse_transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz.functoolz import pipe\n",
    "\n",
    "df[\"locale\"] = df[\"locale\"].apply(lambda x: x.split(\"-\")[0])\n",
    "\n",
    "params = [\n",
    "    remove_punctuation,\n",
    "    drop_cols,\n",
    "    lowercase,\n",
    "]\n",
    "\n",
    "\n",
    "df = pipe(\n",
    "    df,\n",
    "    *params\n",
    ")\n",
    "\n",
    "\n",
    "df, encoder = encode_labels(df)\n",
    "\n",
    "print(f\"Finished preprocessing dataset.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.loc[df['partition'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = [test_df['utt'].values]\n",
    "num_labels = len(df['intent'].unique())\n",
    "labels = [test_df['intent'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "models = [\"xlm-roberta-base\", \"microsoft/mdeberta-v3-base\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_finetuned = {\n",
    "    \"xlm-roberta-base\": \"./models/xlm-roberta-base-finetuned/xlm-roberta-base-finetuned\",\n",
    "    \"microsoft/mdeberta-v3-base\": \"./models/mdeberta-v3-finetuned/mdeberta-MASSIVE-finetuned\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models_finetuned[models[idx]]\n",
    "tokenizer = AutoTokenizer.from_pretrained(models[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for utt_ in utterances:\n",
    "    input_ids_ = []\n",
    "    attention_masks_ = []\n",
    "    for utt in utt_:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            utt,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 128,           # Pad & truncate all sentences.\n",
    "                            truncation = True,\n",
    "                            padding = 'max_length',\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids_.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks_.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    input_ids.append(input_ids_)\n",
    "    attention_masks.append(attention_masks_)\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = [torch.cat(inp, dim=0) for inp in input_ids]\n",
    "attention_masks = [torch.cat(att, dim=0) for att in attention_masks]\n",
    "labels = [torch.tensor(lab) for lab in labels]\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', utterances[0][0])\n",
    "print('Token IDs:', input_ids[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(input_ids[0], attention_masks[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(test_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
